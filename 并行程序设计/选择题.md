# 并行程序设计期末复习个人笔记
根据老师提供题目整理相关知识点，chatGPT辅助

---
1.MPI_Send 和 MPI_recv 是通过____区分消息。( C)  
A.端口号  
B.消息名  
C.消息标签  
D.消息哈希  
MPI 消息想象成一封信，一封信一般包括消息和信封两部分。消息就是寄信人实际要告诉收信人的信息，也就是MPI_Send 函数和MPI_Recv 函数中的消息部分。信封则定义了发送者和接收者的相关信息，让信件能够准确发送和接收。

信封三元素对应的参数为：dest/source、tag、comm。假设s 号调用MPI_Send 函数，而r 号进程调用MPI_Recv 函数。如果s 号进程所发送的消息要能被r 号进程所接收，一般不太严格的要求需要满足以下三个条件：
* s 进程的comm 与r 进程的comm 相等（当两个进程属于同一个通信域，才能彼此通信）
* 同时满足 条件 dest = r 与条件source = s（发送端的dest参数指定了要接收消息进程的进程号，而接收段的source 参数指定了发送消息进程的进程号，两者要配对，否则就接收不到消息）
* s 进程的tag 与 r 进程的tag 相等，（两个进程间的信息标签tag 要相同）

---
2.OpenMP 能实现的是____。( B)  
A.避免数据竞争  
B.提供任务划分策略  
C.确保加速  
D.确保降低通信开销  
在使用 parallel 语句进行累加计算时是通过编写代码来划分任务，再将划分后的任务分配给不同的线程去执行。后来使用 paralle for 语句实现是基于OpenMP 的自动划分，由于存在负载均衡问题，通过子句 schedule 来对影响负载的调度划分方式进行设置。  
* static：静态调度策略
* dynamic：动态调度策略
* guided：指导调度策略
* runtime：运行时调度策略

---
3.对一个串行程序进行 SIMD 并行化,应重点考虑的程序部分是____。( C)  
A.声明语句  
B.条件分支语句  
C.循环语句  
D.输入输出语句  
SIMD（Single Instruction Multiple Data），即对批量的数据同时进行同样的操作以提高效率。

---
4.OpenMP 中指定单线程执行的指令是____。( D)  
A.omp atomic  
B.omp only  
C.omp static  
D.omp single  
原子（atomic），需要保证在当前线程完成操作之前，其它线程不允许操作。  
制导语句 single 和 master 都是指定相关的并行区域只由一个线程执行，区别在于使用 master 则由主线程（0 号线程）执行，使用 single 则由运行时的具体情况决定。  
好像没有另两种指令（

---
5.OpenMP 是___架构下的一种编程工具。( C)  
A.SIMD  
B.MISD  
C.共享内存  
D.分布式内存  
OpenMP共享内存，MPI分布式存储

---
6.四位助教帮助教授批改 300 份试卷，试卷共 16 道题，每位助教负责批改所有试卷的 4 道题，这是一种____任务划分方法。( B)  
A.数据并行  
B.任务并行  
C.搜索并行  
D.预测并行  

1. 数据并行 (Data parallelism)：在深度学习中，训练一个大型神经网络时，可以将数据集划分为多个小批次，并将这些小批次分配给不同的计算设备（如 GPU）进行处理。这样每个设备只需处理一部分数据，从而加速整体训练过程。
2. 任务并行 (Task parallelism)：假设有一个软件开发团队需要完成设计、编码、测试和文档撰写四项任务。我们可以将团队成员按照专长拆分为四组，每组负责其中一项任务，在相同时间内同时工作以提高效率。
3. 搜索并行 (Search parallelism)：在国际象棋引擎中使用搜索树来评估走法。我们可以利用多核心或者多线程的计算资源来同时探索搜索树的不同部分，并在最后合并结果以决定最优走法。
4. 预测并行 (Pipeline parallelism): 在自然语言处理（NLP）领域，当对较长文本进行预测时可采用 Transformer 的微调模型 GPT-3。此场景下可使用预测管道化技术，在某段输入正在被模型前半部分处理时，另一段输入已进入模型后半部分开始处理。通过这种方式实现各阶段预测任务并行执行，提高处理速度。

数据并行主要针对容易拆分且无需交流沟通就能解决问题；而任务并行则关注于同时处理一组独立但有关联的任务，通常涉及协同工作和共享资源。

---
7.超级计算机制造越来越关注的一个新的指标是____。(D)  
A.计算能力  
B.存储能力  
C.占地面积  
D.功耗  

---
8.记并行时间为 T，串行时间为 T'，处理器数量为 p，并行效率 E 的定义是____。(C)  
A.$T'-T$  
B.$\frac{T'}T$  
C.$\frac{T'}{pT}$  
D.$pT-T'$  
线性加速比：$S=\frac{T_{serial}}{T_{parallel}}$  
效率：$E=\frac Sp=\frac{T_{serial}}{pT_{parallel}}$

---
9.关于障碍机制，下面说法错误的是____。(A)
A.会导致快速线程阻塞，不应使用
B.在需要强制线程步调一致时，应使用
C.可用互斥量机制实现
D.属于一种组通信

在多线程编程中必须考虑到不同的线程对同一个变量进行读写访问引起的数据竞争问题。如果线程间没有互斥机制，则不同线程对同一变量的访问顺序是不确定的，有可能导致错误的执行结果。

OpenMP 中有两种不同类型的线程同步机制，一种是互斥机制，一种是事件同步机制。其中事件同步机制的设计思路是控制线程的执行顺序，可以通过设置 barrier 同步路障实现。

在 OpenMP 中调用 barrier 指定语句显式指定栅栏同步时，在 barrier 调用处每个达到此处的线程必须等待其它线程到达，只有当并行区域内的所有线程都达到此处之后，线程才可以继续向下执行。使用 barrier 要注意可能由此引起的死锁问题 。当多个线程之间在逻辑上需要进行交换数据时，可以使用 barrier，强行控制所有线程在此处等待，待数据都准备好后，再继续执行。

---
10.关于 MPI 是什么，以下说法错误的是____。(B)  
A.一种消息传递编程模型标准  
B.一种共享内存编程模型标准  
C.编程角度看是 C++/Fortran 等的库  
D.基于 SPMD 模型  

MPI分布式存储

---
11.动态任务划分相对于静态任务划分的缺点是____。(B)  
A.可能导致负载不均  
B.通信开销高  
C.任务粒度粗  
D.计算复杂度高  
dynamic：动态调度策略，任务/数据在循环执行时被分配给线程。当线
程计算完成时，它能够从运行时系统中请求更多。

---
12.OpenMP 并行模型是一种____模式。(D)  
A.SISD  
B.SIMD  
C.MISD  
D.SPMD  
SISD：单指令流单数据流  
SIMD：单指令流多数据流  
MISD：多指令流单数据流  
MIMD：多指令流多数据流  
SPMD：单程序多份数据进行任务并行，OpenMP和MPI都是这种架构

---
13.下面哪个问题相对而言更不适合进行数据并行____。(B)  
A.求和  
B.排序  
C.向量加法  
D.矩阵乘法  
不太懂，分治算法应该都能并行，快速排序就是一种分治算法

---
14.OpenMP 不会自动地在____位置设置 barrier。(C)  
A.并行结构开始  
B.并行结构结束  
C.其他控制结构开始  
D.其他控制结构结束  

---
15.多个线程进行并行求和，每个线程将自己负责的值依次读入局部变量 x，累加到全局变量sum 上，sum+=x，对此，下面说法正确的是(D)  
A.读取 x 存在数据依赖，不能并发进行  
B.累加顺序被改变，结果是错误的  
C.加法操作是简单运算，无需同步  
D.加法操作不是原子操作，需要同步保证数据依赖  

---
16.在下面问题中,SIMD 并行更适合____。( C)  
A.搜索  
B.排序  
C.矩阵乘法  
D.构建二叉排序树  
SIMD是采用一个指令流处理多个数据流。特点是多个数据采取“相同”操作

---
17.和一对多广播对应的组通信操作是____。( C)  
A.多对一收集  
B.多对多收集  
C.多对一归约  
D.多对多归约  
属于一个进程的数据被发送到通信域中的所有进程，这样的集合通信就叫做广播（broadcast）  
归约就是将消息集中起来处理的方法。归约操作是指对分布在不同进程中的数据间进行交互运算   
散射（MPI_Scatter）与广播非常相似，都是一对多的通信方式，不同的是散射是将一段数据的不同部分发送给所有的进程，而广播则是将相同的数据发送给所有的进程  
归约（Reduction）：
1. 归约操作通常适用于具有结合律和交换律的运算，例如加法、乘法等。
2. 归约过程中每次只处理两个输入元素，并将其合并为一个输出结果。
3. 归约主要目标是在最短时间内得到一个全局结果。

聚集（Aggregation）：
1. 聚集可以应用于更广泛的场景，如统计量计算、自定义函数等。
2. 聚集通常涉及分组操作，在各个分组上执行特定函数后再进行汇总。
3. 聚集关注对数据进行分类或按条件汇总以获得更丰富的信息。

简单来说，归约主要针对可交换与结合的基本运算求全局值；而聚集则广泛应用于跨分组或类别间复杂度较高的变换与处理。

---
18.OpenMP 编译指示中说明私有变量是用____子句。( A)  
A.private  
B.shared  
C.schedule  
D.nowait  

OpenMP 中的变量作用域主要分为以下几类：

1. 全局作用域 (Global scope)：在整个程序中都可见，例如在函数外部定义的静态变量。

2. 线程私有作用域 (Thread-private scope)：每个线程拥有一份独立的变量副本。可以使用 `#pragma omp threadprivate` 指令声明线程私有变量。

3. 并行区域共享作用域 (Shared scope within parallel regions)：在并行区内所有线程都能访问，并且共享一个实例。使用 `#pragma omp parallel` 时，可以通过 `shared` 子句指定共享变量，如：`#pragma omp parallel shared(var)`。

4. 并行区域能够创建的私有（Private）
   - 私有（Private）: 在并行区内为每个线程创建该变量的新副本，并不进行初始化。你可以通过将其包含到 `private` 子句中来声明这样一个私有变量，如：`#pragma omp parallel private(var)`。

5. 归约作用域 (Reduction scope)：归约操作将某种运算应用于各个线程的局部结果，并将其组合成全局结果。使用 `reduction` 子句指定归约变量和操作符，例如求和：`#pragma omp parallel for reduction(+:sum)`。

---
19.MPI 不包括的通信类别是____。(D)
A.点对点通信
B.数据传输组通信
C.计算和数据传输组通信
D.加锁解锁通信

MPI主要包括以下几类通信：

1. 点对点通信（Point-to-point communication）：这是最基本的通信类型，涉及两个进程之间直接发送和接收消息。常用函数有 MPI_Send, MPI_Recv, MPI_Irecv 和 MPI_Isend 等。

2. 集体通信（Collective communication）：集体操作包括多个进程同时进行特定任务的情况。例如广播、归约、散射和聚集等操作。常用函数有
   - MPI_Bcast: 广播数据
   - MPI_Gather: 聚集数据到一个节点上
   - MPI_Scatter: 将数组分发给各节点
   - MPI_Allgather: 所有进程完成聚合，并将结果分配给所有参与者。
   - MPI_Allreduce: 合并数据且在所有参与者中共享结果。
   - 以及其他诸如 Barrier 或 Alltoall 操作等

3. 分组(group)和通讯域(communicator)：MPI 使用 group 和 communicator 的概念来划分和管理处理器子集。通过创建不同的 group 及其相应 communicator，可以在不同范围内执行点对点或集体操作。

---
20.当处理器数量不变时，随着问题规模增大，加速比____。(C)  
A.所有算法都增大  
B.所有算法都减小  
C.代价最优算法都增大  
D.代价最优算法都减小  

如果固定进程/线程的数量，当我们增大问题的规模时，线性加速比和效率都会增加。

---
21.两个 n*n 的矩阵相乘，将所有 n^2 个乘法计算划分给不同进程，再将对应某行某列的 n个乘法结果累加得到结果矩阵对应元素，这是一种划分____的数据并行。(B)  
A.输入数据  
B.中间结果  
C.输出数据  
D.临时数据  

---
22.下列那个不是集合通信的一般功能： ( D)  
A.通信功能  
B.聚集功能  
C.同步功能  
D.异步功能  

---
23.下列哪项不属于并行数据划分方式： ( B)  
A.块划分  
B.堆划分  
C.循环划分  
D.块-循环划分  
并行数据划分方式主要有以下几种：  
块划分：数据被均匀地分成若干个连续的块，每个处理器负责一个或多个块。

循环划分：将数据元素按照固定间隔进行循环和周期性地指派给各个处理器。

块-循环划分：是块划分和循环划分的结合，先对数据进行大致相等大小的块平衡后再进行循环调度到各个处理器上。

而堆（Heap）通常与内存管理相关，并不属于并行数据划分方式。

---
24.数据并行模型有以下哪个特性： ( A)  
A.并行工作主要是操纵数据集。  
B.任务集都使用不相同的数据结构  
C.每个任务的工作都是不同的，  
D.在共享内存体系结构上，不是所有的任务都是在全局存储空间中访问数据。  
数据并行是相同指令操纵相同结构的数据

---
25.关于 OpenMP 的特点下列哪项是不正确的： (D)  
A.应用编程接口 API（Application Programming Interface ）  
B.是 C/C++ 和 Fortan 等的应用编程接口  
C.已经被大多数计算机硬件和软件厂家所标准化  
D.建立在分布式存储系统上的  

---
26.Foster 并行算法中的四步过程是：（C）  
A.划分、映射、通信、同步  
B.划分、聚集、通信、映射  
C.划分、通信、聚集、映射  
D.划分、通信、聚集、同步  

---
27.下面对于共享内存描述错误的是：（D）  
A.并行计算机的存储方式一般可分为共享内存和分布式内存  
B.在分布式内存的并行计算中,各处理单元都拥有自己独立的局部存储器。  
C.共享内存架构限制了并行计算集群规模的并一步扩展  
D.共享内存架构的信息交互速度要弱于分布式内存。  
共享内存取决与硬件速度，分布式内存取决于网络速度，一般硬件速度大于网络速度

---
28.下面对于计算和通信之间的关系错误的是：（B）  
A.对于并行计算，有一个很重要的原则就是设法加大计算时间相对于通信时间的比重，甚至以更大的计算量换取更少的通信量。  
B.并行计算过程中，应该避免通信和计算的重叠，减小错误的发生概率。  
C.SIMD 并行计算机一般适合同步并行算法 而 MIMD 并行计算机则适合异步并行算法。  
D.对于同步并行算法而言,任务的各个部分是同步向前推进的。有一个全局的时钟来控制各部分的运行步调。  
并行是一边通信一边计算

---
29.下列关于 Cache 一致性问题的说法中，正确的是：（B）  
A.当一个处理更新 Cache 中某变量的值时，其它处理器不必立即更新，需要经过一个静默判断期。  
B.监听 Cache 一致性协议中，每更新一次变量就需要进行一次广播。
C.在大型系统中，监听 Cache 一致性问题解决起来相对更加容易。
D. CPU Cache 虽是由系统硬件进行管理的，但程序员可以直接操作读写。

监听 Cache 一致性协议（如 MESI 和 MOESI 协议）是为了解决多处理器系统中缓存一致性问题而采用的方法。在这些协议中，当一个处理器更新其缓存中某变量的值时，需要广播该操作以通知其他处理器要么更新它们自己的缓存副本，要么使其失效。这样可以确保所有处理器看到相同数据，并保持各个缓存间的一致性。

---
30.下列关于数据并行和任务并行的说法中，正确的是：（A）  
A.数据并行模式中，不同的处理单元具有取得相同数据的权限。  
B.任务并行模式中，不同处理单元执行相同的任务。  
C.在大型的复杂程序中，数据并行模式通常具有更好的性能表现。  
D.如果两项任务之间具有数据依赖性，则必须采用任务并行模式设计程序。  
数据并行相同任务相同数据，任务并行不同任务不同数据

如果有数据依赖，还可能是必须顺序执行

---
31.下列关于 MPI 的说法中，正确的是：（B）  
A.MPI 程序所需的并行资源在编译前已经完成预留。  
B.MPI_Init 与 MPI_Finalize 函数通常多用于 main 函数中。
C.在同一并行程序中不能同时存在多个通信子。  
D.位于 MPI_Init 与 MPI_Finalize 之间的各行代码一定是并行执行的  

A. 错误。MPI 程序的并行资源是在运行时分配的，而不是在编译前预留。

C. 错误。同一并行程序中可以存在多个通信子（communicator）。通信子用于定义进程组和规定它们之间的通信上下文。默认情况下，所有 MPI 进程都属于全局通信子 MPI_COMM_WORLD，但用户也可以创建自己的自定义通信子。

D. 错误。位于 MPI_Init 和 MPI_Finalize 之间的代码，并不意味着全部都是并行执行的。这两个函数只是初始化和终止 MPI 库所需功能及资源；具体哪些部分实际上以并行方式执行取决于如何编写代码以及使用了哪些特定MPI 函数进行数据传递、任务划分等操作

---
32.下列关于 MPI_Send 的说法中，不正确的是：（c）  
A.MPI_Send 再次调用时并不能确定上次调用时的数据是否已经发送。  
B.MPI_Send 一定需要与 MPI_Recv 配对。  
C.1 号进程在 2 号进程之前调用 MPI_Send，则 MPI 同步规则要求要求 1 号进程发送的数据一定要先于 2 号进程到达，否则需要进行缓冲。  
D.MPI_Send 函数定义了 6 个参数。  
这个得看程序实现吧，如果是MPI_ANY_SOURCE就可以进程在接
收消息时按照进程完成工作的顺序，而不是按代码里固定的顺序

---
33.如果 MPI 中发生了进程“悬挂”现象，可能是由下列哪种情况造成的：（C）  
A.MPI_Send 的第二项参数设置错误  
B.MPI_Send 的第三项参数设置错误  
C.MPI_Send 的第四项参数设置错误  
D.MPI_Send 所有六项参数中，任意项设置错误都会导致悬挂  

MPI_Send的6项参数：buf（指向包含消息内容的内存块的指针），count（数据多少，比如3个MPI_INT） ，datatype（数据类型），dest（目标进程），tag（消息标签），comm（通信域）

目标进程设置错误，会导致接受不到信息，进程阻塞

---
34.MPI 信息的不可超越性是指在程序设计时要保证：（C）  
A.一定要保证先调用 MPI_Send 的进程，传送的数据先到达目的地。  
B.多个进程调用 MPI_Send 时，编号小的进程调用必须享有优先权。  
C.同一进程连续两次调用 MPI_Send 发送数据给另一进程，则必须保证第一次发送的数据接收成功后再接收第二次发送的数据。  
D.同一进程连续两次调用 MPI_Send 发送数据给另一进程，则必须保证第一次发送的数据接收成功后再进行第二次发送。  

MPI 允许进行异步通信，即一个进程可以同时发出多个消息而无需等待每个消息被接收。

C 选项正确地描述了信息的不可超越性：当同一个进程连续两次向另一个进程发送数据时，在接收端必须按照发送的顺序依次接收这些数据。换言之，在接收端不能跳过先前还未处理完毕的消息直接处理后来者。

---
35.一次成功的 MPI_Reduce 调用中不需要保证的是：（B）  
A.每个源进程的 dest_process 必须是相同的。  
B.每个源进程传送的数据必须是单个的标量。  
C.每个源进程的 Operator 必须是相同的。  
D.每个源进程的通信子必须是相同的  

可以是更复杂的数据

---
36.下列关于集体通信函数的概念中正确的是：（B）  
A.MPI_Bcasth 函数支持从任意进程向全体进程发送。  
B.MPI_Scatter 函数支持从任意进程给向全体进程发送。  
C.MPI_Reduce 函数是 MPI_Scatter 函数的逆过程  
D.MPI_Scatter 函数可以使用循环划分法进行划分  
规约（Reduce），散射（Scatter），聚集（Gather）

---
37.假设一个程序的串行版本中，总运行时间为 120s，其中 90%可并行化。现有 4 个核，假设程序的并行设计是理想化的，则概念中，正确的是：（B）  
A.程序的理想化的并行运行总时间是 27s  
B.程序的理想线性加速比约为 3.07  
C.程序的理想效率为 1.11  
D.阿姆达尔定律指出，当核数足够大时该程序拥有的理论加速比上限为 15  
总时间是120*90%/4+120*10%=39    
线性加速比是120/39=3.07  
效率是120/(39*4)=0.769  
阿姆达尔定律：无法并行部分占比为r，加速比上限为1/r  

---
38.下列关于 OpenMP 的概念中，不正确的是：（C）  
A.OpenMP 中设置临界区原因是多个线程需要同时访问并更新共享变量  
B.OpenMP 架构的扩展性潜力弱于 MPI  
C.同一项目中，OpenMP 结构的通信时间明显大于 MPI  
D.parallel for 之后的结构化代码块必须是可循环执行的  
内存通信时间小于网络通信时间

---
39.如果设计时发现循环之间出现数据依赖，可能采用的并行方法是：（C）  
A.无法使用 OpenMP 完成并行  
B.可尝试其它数据结构，以在循环之间消除数据依赖  
C.可尝试在每个循环之内寻找并行模式，并将整个循环调度给一个线程  
D.可尝试在循环之间设置显式障碍，以在循环之间消除数据依赖  

---
40.研究并行算法的可扩展性的目的是：（c）  
①确定某类问题的何种并行算法与何种并行系统的组合，可有效的利用系统大量处理机  
②有算法在小规模并行机上的运行性能来预测起移植到大规模并行机上后的运行性能  
③对一固定规模的问题，能确定起运行在某类并行系统上时，所需的最有处理机台数和获得的最大加速比  
④指导并行算法和并行机体系结构的改进  
A.①②③  
B.②③④  
C.①②③④  
D.①③④  

---
41.并行算法按照进程间程序的执行顺序关系可以分为同步算法、异步算法和 ( A )  
A.独立并行算法  
B.混合算法  
C.加速算法  
D.级联算法  

---
42.常用的三个加速比性能模型包括( D )。  
①固定负载加速比性能模型 ②固定时间加速比性能模型  
③存储受限加速比性能模型 ④固定速度加速比性能模型  
A.①②④  
B.①③④  
C.②③④  
D.①②③  

---
43.MPI 提供哪种机制来处理进程间的通信不同步问题？( B )  
A.同步通信  
B.非阻塞通信  
C.集合通信  
D.以上所有选项  

---
44.下面哪个选项描述了 MPI 的非阻塞通信？( A )。  
A. 发送或接收操作开始后，用户程序可以继续执行，直到通信结束  
B. 发送或接收操作开始后，用户程序需要等待通信结束  
C. 发送或接收操作开始后，用户程序不必等待通信结束  
D. 发送或接收操作开始后，用户程序必须立即结束   

非阻塞强调线程在遇到无法立即获得结果的情况下不进入长时间等待

---
45.下列哪个指令是 OpenMP 中用于并行化循环的？（B）  
A. #pragma omp parallel  
B. #pragma omp for  
C. #pragma omp sections   
D. #pragma omp single  

---
46.OpenMP 中的“私有数据”是什么意思？（B）  
A. 只能由一个线程访问的数据  
B. 只能由创建它的线程访问的数据  
C. 只能在并行区域内访问的数据  
D. 以上所有选项  

---
47.并行计算中，哪项是减少通信开销的有效策略？（C）  
A. 使用更高效的编程模型  
B. 提高处理器的计算能力  
C. 尽可能在局部处理数据  
D. 增加更多的处理器  

处理完了就减少通信量

---
48．在并行编程中，哪个选项不是解决数据依赖性问题的方法？（D）  
A. 更改程序顺序  
B. 使用私有变量  
C. 利用任务并行性  
D. 增加数据通信  

正是因为通信造成多个线程都要读写，才会有依赖问题

---
49.在并行编程中，下列哪种情况更有可能导致“竞态条件”发生？（D）  
A. 多个线程读取同一数据  
B. 多个线程写入同一数据  
C. 一个线程读取数据，另一个线程写入数据  
D. B 和 C 都正确  

改变数据使得不同端获取的数据不同，更有可能竞态

---
50.在 OpenMP 中，哪一个 pragma 能够保护一个代码块，使其在同一时间只能由一个线程执行？( C)  
A. #pragma omp parallel  
B. #pragma omp for  
C. #pragma omp critical  
D. #pragma omp sections  

编译制导critical 声明一个临界区，临界区一次只允许一个线程执行，因此它在并行代码提供了一种串行的机制。